from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the datafile from the location
team_sts = spark.read.csv("/FileStore/tables/game_teams_stats.csv", header = True, inferSchema = True)
# drop the unwanted columns
team_sts = team_sts.drop("powerPlayOpportunities","faceOffWinPercentage","giveaways","takeaways")
team_sts.show(5)

-------------------------------------------------------------------------------------------------------------------------------


from pyspark.sql import SparkSession
from pyspark.sql.types import *

ds = [StructField("game_id", IntegerType(),True),StructField("team_id", IntegerType(),True),StructField("HoA", StringType(),True),
     StructField("won", StringType(),True),StructField("settled_in", StringType(),True),StructField("head_coach", StringType(),True),
     StructField("goals", IntegerType(),True),StructField("shots", IntegerType(),True),StructField("hits", IntegerType(),True),
     StructField("pim", IntegerType(),True),StructField("powerPlayGoals", IntegerType(),True)]

fds = StructType(fields = ds)

# read the datafile from the location
team_sts = spark.read.csv("/FileStore/tables/game_teams_stats.csv", header = True, schema = fds)
# drop the unwanted columns
team_sts = team_sts.drop("powerPlayOpportunities","faceOffWinPercentage","giveaways","takeaways")
team_sts.show(5)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the datafile from the location
team = spark.read.csv("/FileStore/tables/team_info.csv", header = True, inferSchema = True)

team.show(5)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

ds = [StructField("game_id", IntegerType(),True),StructField("team_id", IntegerType(),True),StructField("HoA", StringType(),True),
     StructField("won", StringType(),True),StructField("settled_in", StringType(),True),StructField("head_coach", StringType(),True),
     StructField("goals", IntegerType(),True),StructField("shots", IntegerType(),True),StructField("hits", IntegerType(),True),
     StructField("pim", IntegerType(),True),StructField("powerPlayGoals", IntegerType(),True)]

fds = StructType(fields = ds)

# read the datafile from the location
team_sts = spark.read.csv("/FileStore/tables/game_teams_stats.csv", header = True, schema = fds)
# drop the unwanted columns
team_sts = team_sts.drop("powerPlayOpportunities","faceOffWinPercentage","giveaways","takeaways")
team_sts.createOrReplaceTempView("Team_Stat")


# read the datafile from the location
team = spark.read.csv("/FileStore/tables/team_info.csv", header = True, inferSchema = True)
team.createOrReplaceTempView("Team")

spark.sql("select ts.game_id, ts.team_id, ts.HoA, ts.won, tm.teamName from Team_Stat ts join Team tm on ts.team_id = tm.team_id").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)
# drop the unwanted columns
game = game.drop("type","date_time","away_team_id",	"home_team_id","venue_link", "venue_time_zone_id", "venue_time_zone_offset", "venue_time_zone_tz")

game.createOrReplaceTempView("Games")
spark.sql("select * from Games").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

ds = [StructField("game_id", IntegerType(),True),StructField("team_id", IntegerType(),True),StructField("HoA", StringType(),True),
     StructField("won", StringType(),True),StructField("settled_in", StringType(),True),StructField("head_coach", StringType(),True),
     StructField("goals", IntegerType(),True),StructField("shots", IntegerType(),True),StructField("hits", IntegerType(),True),
     StructField("pim", IntegerType(),True),StructField("powerPlayGoals", IntegerType(),True)]

fds = StructType(fields = ds)

# read the datafile from the location
team_sts = spark.read.csv("/FileStore/tables/game_teams_stats.csv", header = True, schema = fds)
# drop the unwanted columns
team_sts = team_sts.drop("powerPlayOpportunities","faceOffWinPercentage","giveaways","takeaways")
team_sts.createOrReplaceTempView("Team_Stat")


# read the datafile from the location
team = spark.read.csv("/FileStore/tables/team_info.csv", header = True, inferSchema = True)
team.createOrReplaceTempView("Team")

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)
# drop the unwanted columns
game = game.drop("type","date_time","away_team_id",	"home_team_id","venue_link", "venue_time_zone_id", "venue_time_zone_offset", "venue_time_zone_tz")
game.createOrReplaceTempView("Games")

spark.sql("select ts.game_id, gm.venue, gm.season, substr(gm.season,1,4) as year, substr(gm.season,7,8) as date,ts.team_id, ts.HoA, ts.won, tm.teamName from Team_Stat ts join Team tm on ts.team_id = tm.team_id join games gm on gm.game_id = ts.game_id").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import *

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)
game.createOrReplaceTempView("Game")

# read the datafile from the location
game_shift = spark.read.csv("/FileStore/tables/game_shifts_info.csv", header = True, inferSchema = True)
game_shift.createOrReplaceTempView("Game_Shift")

df = spark.sql("select gm.game_id, gm.season, gm.type, gm.away_goals, gm.home_goals, gm.outcome, gm.venue, gm.home_rink_side_start, gs.player_id, gs.shift_start, gs.shift_end from game gm join game_shift gs on gm.game_id = gs.game_id")

df.show()# 498265 is the record count

-------------------------------------------------------------------------------------------------------------------------------

