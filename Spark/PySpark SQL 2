from pyspark.sql import SparkSession

file_location = "/FileStore/tables/game_skater_stats_spark.csv"
# Create a data frame in spark
df = SparkSession.builder.appName("Game_Skater_Stats").getOrCreate()

# read the datafile from the location
df = spark.read.format("csv").option("inferSchema", 
           True).option("header", True).load(file_location)
# df.show(5)

# save the content in parquet format
# df.write.save('/FileStore/tables/FileStore/tables/game_skater_stats_spark',  format='parquet')

df.write.format("parquet").mode("overwrite").options(header="true").save("/FileStore/tables/FileStore/game_skater_stats_spark")
			   
-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

# Read the parquet file from the below location
data = sqlContext.read.parquet("/FileStore/tables/FileStore/game_skater_stats_spark/*.parquet")

display(data)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

# Read the dataset in parquet format
df = spark.read.load("/FileStore/tables/FileStore/game_skater_stats_spark/*.parquet")

# Create a Spark SQL Temp Table to store the parquet datafile
df.createOrReplaceTempView("NHL_Skater_Stats")

res = spark.sql("select player_id, sum(shots) as shots, sum(assists) as assists, count(game_id) as game_id, sum(hits) as hits from NHL_Skater_Stats group by player_id order by hits desc")
display(res)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
# Read the dataset from parquet file
df = spark.read.load("/FileStore/tables/FileStore/player_analysis/*.parquet").sort("hits", ascending = False)
display(df)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
import pyspark.sql.functions as f

# Read the dataset
df = spark.read.load("/FileStore/tables/FileStore/game_skater_stats_spark")

gd = df.groupBy("player_id")
res = gd.agg({'shots':'sum','assists': 'sum','game_id': 'count','hits':'sum'}).sort("sum(hits)", ascending = False)

# Rename the column using withColumnRenamed() function
df2 = res.withColumnRenamed("sum(hits)","hits") \
    .withColumnRenamed("sum(assists)","assists") \
    .withColumnRenamed("sum(shots)","shots") \
    .withColumnRenamed("count(game_id)","game_id")


display(df2)

# Save the output in parquet format
df2.write.format("parquet").mode("overwrite").options(header="true").save("/FileStore/tables/FileStore/player_analysis")

-------------------------------------------------------------------------------------------------------------------------------

