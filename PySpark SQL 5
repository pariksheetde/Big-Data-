from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import *
from pyspark.sql.window import Window

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)
game.createOrReplaceTempView("Game")

# read the datafile from the location
game_shift = spark.read.csv("/FileStore/tables/game_shifts_info.csv", header = True, inferSchema = True)
game_shift.createOrReplaceTempView("Game_Shift")

# read the datafile from the location
player_info = spark.read.csv("/FileStore/tables/player_info.csv", header = True, inferSchema = True)
player_info.createOrReplaceTempView("Player_Info")

# read the datafile from the location
game_player_info = spark.read.csv("/FileStore/tables/game_plays_players_info.csv", header = True, inferSchema = True)
game_player_info.createOrReplaceTempView("Game_Player_Info")

df = spark.sql("select row_number() over (order by sum(away_goals) desc) as row_number, rank() over (partition by gm.season order by sum(away_goals) desc) as rank, gs.player_id, gpi.playerType, pi.firstname, pi.lastname, gm.game_id, gm.season, sum(away_goals) as away_goals, sum(home_goals) as home_goals from game gm join game_shift gs on gm.game_id = gs.game_id join player_info pi on pi.player_id = gs.player_id join Game_Player_Info gpi on gpi.player_id = pi.player_id group by 3,4,5,6,7,8")

df.show()

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import *
from pyspark.sql.window import Window

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)
game.createOrReplaceTempView("Game")

# read the datafile from the location
game_shift = spark.read.csv("/FileStore/tables/game_shifts_info.csv", header = True, inferSchema = True)
game_shift.createOrReplaceTempView("Game_Shift")

# read the datafile from the location
player_info = spark.read.csv("/FileStore/tables/player_info.csv", header = True, inferSchema = True)
player_info.createOrReplaceTempView("Player_Info")

# read the datafile from the location
game_player_info = spark.read.csv("/FileStore/tables/game_plays_players_info.csv", header = True, inferSchema = True)
game_player_info.createOrReplaceTempView("Game_Player_Info")

df = spark.sql("select row_number() over (order by sum(away_goals) desc) as row_number, rank() over (partition by gm.season order by sum(away_goals) desc) as rank, gs.player_id, gpi.playerType, pi.firstname, pi.lastname, gm.game_id, gm.season, sum(away_goals) as away_goals, sum(home_goals) as home_goals from game gm join game_shift gs on gm.game_id = gs.game_id join player_info pi on pi.player_id = gs.player_id join Game_Player_Info gpi on gpi.player_id = pi.player_id where playerType = 'Winner' group by 3,4,5,6,7,8")

df.show()

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import *
from pyspark.sql.window import Window

# read the datafile from the location
game_skater_stats = spark.read.csv("/FileStore/tables/game_skater_stats.csv", header = True, inferSchema = True)

# drop the unwanted columns
game_skater_stats = game_skater_stats.drop("penaltyMinutes",	"faceOffWins",	"faceoffTaken",	"takeaways",	"giveaways",	"shortHandedGoals",	"shortHandedAssists",	"blocked",	"plusMinus",	"evenTimeOnIce",	"shortHandedTimeOnIce",	"powerPlayTimeOnIce")
game_skater_stats.createOrReplaceTempView("game_skater_stats")

# read the datafile from the location
team_info = spark.read.csv("/FileStore/tables/team_info.csv", header = True, inferSchema = True)
team_info.createOrReplaceTempView("team_info")

spark.sql("select * from (select gss.game_id, ti.franchiseId, ti.teamName,ti.abbreviation, gss.player_id, sum(gss.assists) over (partition by gss.game_id order by gss.game_id) as assists, sum(gss.goals) over (partition by gss.game_id order by gss.game_id) as goals, sum(gss.shots) over (partition by gss.game_id order by gss.game_id) as shots, sum(gss.hits) over (partition by gss.game_id order by gss.game_id) as hits from game_skater_stats gss join team_info ti on gss.team_id = ti.team_id) a").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import *
from pyspark.sql.window import Window

# read the datafile from the location
game_skater_stats = spark.read.csv("/FileStore/tables/game_skater_stats.csv", header = True, inferSchema = True)

# drop the unwanted columns
game_skater_stats = game_skater_stats.drop("penaltyMinutes",	"faceOffWins",	"faceoffTaken",	"takeaways",	"giveaways",	"shortHandedGoals",	"shortHandedAssists",	"blocked",	"plusMinus",	"evenTimeOnIce",	"shortHandedTimeOnIce",	"powerPlayTimeOnIce")
game_skater_stats.createOrReplaceTempView("game_skater_stats")

# read the datafile from the location
team_info = spark.read.csv("/FileStore/tables/team_info.csv", header = True, inferSchema = True)
team_info.createOrReplaceTempView("team_info")

# read the datafile from the location
player_info = spark.read.csv("/FileStore/tables/player_info.csv", header = True, inferSchema = True)
player_info.createOrReplaceTempView("player_info")

spark.sql("select * from (select gss.game_id, ti.franchiseId, ti.teamName,ti.abbreviation, gss.player_id, pi.firstname, pi.lastname, sum(gss.assists) over (partition by gss.game_id order by gss.game_id) as assists, sum(gss.goals) over (partition by gss.game_id order by gss.game_id) as goals, sum(gss.shots) over (partition by gss.game_id order by gss.game_id) as shots, sum(gss.hits) over (partition by gss.game_id order by gss.game_id) as hits from game_skater_stats gss join team_info ti on gss.team_id = ti.team_id join player_info pi on pi.player_id = gss.player_id) a").show(10)

-------------------------------------------------------------------------------------------------------------------------------
