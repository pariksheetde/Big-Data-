from pyspark.sql import SparkSession
from pyspark.sql.types import *

ds = [StructField("player_id", IntegerType(),True),StructField("firstname", StringType(),True),StructField("lastname", StringType(),True),
     StructField("nationality", StringType(),True),StructField("birthCity", StringType(),True),StructField("primaryPosition", StringType(),True),
     StructField("birthdate", DateType(),True),StructField("link", StringType(),True)]
fds = StructType(fields = ds)
# read the datafile from the location
player = spark.read.csv("/FileStore/tables/player_info.csv", header = True, schema = fds)
# drop the unwanted columns
player = player.drop("primaryPosition","link")
player.show(5)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

ds = [StructField("player_id", IntegerType(),True),StructField("firstname", StringType(),True),StructField("lastname", StringType(),True),
     StructField("nationality", StringType(),True),StructField("birthCity", StringType(),True),StructField("primaryPosition", StringType(),True),
     StructField("birthdate", DateType(),True),StructField("link", StringType(),True)]
fds = StructType(fields = ds)
# read the datafile from the location
player = spark.read.csv("/FileStore/tables/player_info.csv", header = True, schema = fds)
# drop the unwanted columns
player = player.drop("primaryPosition","link")
player.createOrReplaceTempView("Player_Info")

fdf = spark.sql("select player_id, firstname, lastname, nationality, birthcity, to_date(birthdate) as birthdate, dayofmonth(birthdate) as date, month(birthdate) as month, year(birthdate) as year from player_info").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

gsi_ds = [StructField("game_id", IntegerType(),True),StructField("player_id", IntegerType(),True),StructField("period", IntegerType(),True),
     StructField("shift_start", IntegerType(),True),StructField("shift_end", IntegerType(),True)]
gsi = StructType(fields = gsi_ds)
# read the datafile from the location
player = spark.read.csv("/FileStore/tables/game_shifts_info.csv", header = True, schema = gsi)

player.createOrReplaceTempView("Geme_Shift_Info")


pi_ds = [StructField("player_id", IntegerType(),True),StructField("firstname", StringType(),True),StructField("lastname", StringType(),True),
     StructField("nationality", StringType(),True),StructField("birthCity", StringType(),True),StructField("primaryPosition", StringType(),True),
     StructField("birthdate", DateType(),True),StructField("link", StringType(),True)]
pi = StructType(fields = pi_ds)
# read the datafile from the location
player = spark.read.csv("/FileStore/tables/player_info.csv", header = True, schema = pi)
# drop the unwanted columns
player = player.drop("primaryPosition","link")
player.createOrReplaceTempView("Player_Info")


df = spark.sql("select pl.player_id, gm.game_id, pl.firstname, pl.lastname, pl.nationality, pl.birthCity, gm.period, gm.shift_start, gm.shift_end from Geme_Shift_Info gm join Player_Info pl on pl.player_id = gm.player_id").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)

game = game.drop("type","date_time_GMT","away_team_id","home_team_id","away_goals","home_goals","venue_link","venue_time_zone_id","venue_time_zone_offset","venue_time_zone_tz")
game.createOrReplaceTempView("Game_Info")
df = spark.sql("select game_id, season, date_time, dayofmonth(date_time) as date, month(date_time) as month, year(date_time) as year, initcap(home_rink_side_start) as home_rink_side_start from Game_Info").show(10)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the datafile from the location
game = spark.read.csv("/FileStore/tables/game.csv", header = True, inferSchema = True)

game = game.drop("type","date_time_GMT","away_team_id","home_team_id","away_goals","home_goals","venue_link","venue_time_zone_id","venue_time_zone_offset","venue_time_zone_tz")
game.createOrReplaceTempView("Game_Info")
df = spark.sql("select game_id, season, date_time, dayofmonth(date_time) as date, month(date_time) as month, year(date_time) as year, initcap(home_rink_side_start) as home_rink_side_start from Game_Info where lower(home_rink_side_start) = 'right'").show(10)

# Save the output in parquet format
game.write.format("parquet").mode("overwrite").options(header="true").save("/FileStore/tables/FileStore/Game_Info")

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.types import *

# read the parquet datafile from the location
game = spark.read.parquet("/FileStore/tables/FileStore/Game_Info/*.parquet")
game.createOrReplaceTempView("Games")
df = spark.sql("select * from Games").show(3)

-------------------------------------------------------------------------------------------------------------------------------

from pyspark.sql import SparkSession

# Read the dataset in parquet format
df = spark.read.load("/FileStore/tables/FileStore/Game_Info/*.parquet")
game.createOrReplaceTempView("Games_Dtls")
df = spark.sql("select * from Games_Dtls where lower(home_rink_side_start) = 'left'").show(10)

-------------------------------------------------------------------------------------------------------------------------------
